{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GerardAlba/UdAAnaliticaFinancera/blob/main/UdAForecasting_Inflation_with_ARIMA_and_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Starter Notebook on Forecasting Inflation"
      ],
      "metadata": {
        "id": "fGOLQbdO4uq4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Navigation\n",
        "I. <b>Import Libraries and Data Loading</b><br>\n",
        "a) [Import Libraries and Data Loading](#libraries)<br><br>\n",
        "\n",
        "II. <b> Data Understanding</b><br>\n",
        "a) [Macroeconomic Trends](#trend)<br>\n",
        "b) [Core CPI trend by Month and Quarter](#mq)<br><br>\n",
        "\n",
        "III. <b> Forecast</b><br>\n",
        "a) [ARIMA](#arima)<br>\n",
        "b) [Univariate Forecasting with LSTM](#ulstm)<br>\n",
        "c) [Multivariate Forecasting with LSTM](#mlstm)<br>\n"
      ],
      "metadata": {
        "id": "aJAsUCEa4uq6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Import Libraries and Data Loading\n",
        "<a id=\"libraries\"></a>"
      ],
      "metadata": {
        "id": "LDwOPqJL4uq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import plotly.express as px\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "init_notebook_mode(connected=True)\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.tsa.stattools import grangercausalitytests\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from math import sqrt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "np.random.seed(234)\n",
        "tf.random.set_seed(234)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:00:53.297362Z",
          "iopub.execute_input": "2022-01-09T06:00:53.297836Z",
          "iopub.status.idle": "2022-01-09T06:00:53.399352Z",
          "shell.execute_reply.started": "2022-01-09T06:00:53.297801Z",
          "shell.execute_reply": "2022-01-09T06:00:53.398244Z"
        },
        "trusted": true,
        "id": "_8-0qmif4uq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.__version__)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:00:53.405287Z",
          "iopub.execute_input": "2022-01-09T06:00:53.405579Z",
          "iopub.status.idle": "2022-01-09T06:00:53.411893Z",
          "shell.execute_reply.started": "2022-01-09T06:00:53.405547Z",
          "shell.execute_reply": "2022-01-09T06:00:53.411106Z"
        },
        "trusted": true,
        "id": "4fjBBySy4uq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "monthly_raw = pd.read_csv('../input/usa-key-macroeconomic-indicators/macro_monthly.csv',parse_dates=True)\n",
        "monthly_raw.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:00:53.413265Z",
          "iopub.execute_input": "2022-01-09T06:00:53.413543Z",
          "iopub.status.idle": "2022-01-09T06:00:53.434914Z",
          "shell.execute_reply.started": "2022-01-09T06:00:53.413512Z",
          "shell.execute_reply": "2022-01-09T06:00:53.434333Z"
        },
        "trusted": true,
        "id": "rNCTvF1e4uq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets take from 1994 onwards, as there are some NaNs for some of the indicators.\n",
        "We can do so by simply dropping the NaNs."
      ],
      "metadata": {
        "id": "TTubikp94uq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "monthly_raw.dropna(inplace=True)\n",
        "monthly_raw.head(5)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:00:53.436327Z",
          "iopub.execute_input": "2022-01-09T06:00:53.43693Z",
          "iopub.status.idle": "2022-01-09T06:00:53.458158Z",
          "shell.execute_reply.started": "2022-01-09T06:00:53.436897Z",
          "shell.execute_reply": "2022-01-09T06:00:53.457483Z"
        },
        "trusted": true,
        "id": "pnk5PHxf4uq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Understanding\n",
        "Quick check on data types and convert realtime_start and date to datetime."
      ],
      "metadata": {
        "id": "1FpiQvWs4uq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "monthly_raw.dtypes\n",
        "\n",
        "monthly_raw.DATE = pd.to_datetime(monthly_raw.DATE)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:00:53.459358Z",
          "iopub.execute_input": "2022-01-09T06:00:53.459841Z",
          "iopub.status.idle": "2022-01-09T06:00:53.47521Z",
          "shell.execute_reply.started": "2022-01-09T06:00:53.459801Z",
          "shell.execute_reply": "2022-01-09T06:00:53.474534Z"
        },
        "trusted": true,
        "id": "FR1Wb33p4uq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check number of month/year in dataset"
      ],
      "metadata": {
        "id": "0s4TBDHD4uq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "monthly_raw['DATE'].nunique()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:00:53.476484Z",
          "iopub.execute_input": "2022-01-09T06:00:53.477295Z",
          "iopub.status.idle": "2022-01-09T06:00:53.484202Z",
          "shell.execute_reply.started": "2022-01-09T06:00:53.477248Z",
          "shell.execute_reply": "2022-01-09T06:00:53.483439Z"
        },
        "trusted": true,
        "id": "Mtp4UxNQ4uq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a copy"
      ],
      "metadata": {
        "id": "v8A2do5o4uq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "monthly_df = monthly_raw.copy()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:00:53.485298Z",
          "iopub.execute_input": "2022-01-09T06:00:53.485864Z",
          "iopub.status.idle": "2022-01-09T06:00:53.49401Z",
          "shell.execute_reply.started": "2022-01-09T06:00:53.485828Z",
          "shell.execute_reply": "2022-01-09T06:00:53.493322Z"
        },
        "trusted": true,
        "id": "P9rv8Wfl4uq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Macroeconomic Indicator Trend\n",
        "<a id = \"trend\"></a>\n",
        "We can perform feature generation first and create percentage change Month-over-month and Year-over-year."
      ],
      "metadata": {
        "id": "AdA1HsXk4uq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "monthly_df['cpi_pct_mom'] = round((monthly_df['ccpi'].pct_change().fillna(0))*100,2)\n",
        "monthly_df['cpi_pct_yoy'] = round((monthly_df['ccpi'].pct_change(12).fillna(0))*100,2)\n",
        "\n",
        "monthly_df.iloc[:, 1:13].plot(kind ='line',\n",
        "            subplots = True,\n",
        "            figsize = (16,16),\n",
        "            title = ['Unemployment Rate', 'Personal Saving Rate','M2','Disposable Income','Personal Consumption Expenditure','Real Effective Exchange Rate',\n",
        "                     '10Y Treasury Yield','Fed Rate','Construction Spending','Industrial Production Index','Core CPI','Core CPI % Change MoM'],\n",
        "            legend = False,\n",
        "            layout = (4,3),\n",
        "            sharex = True,\n",
        "            style = ['midnightblue', 'steelblue', 'dodgerblue', 'slateblue','mediumblue','darkslateblue','red','salmon','brown','maroon','tomato'])\n",
        "\n",
        "plt.suptitle('27 Year Macroeconomic Indicators for the United States', fontsize = 24)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:00:53.494998Z",
          "iopub.execute_input": "2022-01-09T06:00:53.495772Z",
          "iopub.status.idle": "2022-01-09T06:00:55.627062Z",
          "shell.execute_reply.started": "2022-01-09T06:00:53.495733Z",
          "shell.execute_reply": "2022-01-09T06:00:55.626192Z"
        },
        "trusted": true,
        "id": "opeCDHZV4uq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Core CPI trend by Month and Quarter\n",
        "<a id = \"mq\"></a>"
      ],
      "metadata": {
        "id": "g5Kb33EX4uq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "monthly_df['year'] = monthly_df['DATE'].apply(lambda x: x.year)\n",
        "monthly_df['quarter'] = monthly_df['DATE'].apply(lambda x: x.quarter)\n",
        "monthly_df['month'] = monthly_df['DATE'].apply(lambda x: x.month)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:00:55.628205Z",
          "iopub.execute_input": "2022-01-09T06:00:55.628414Z",
          "iopub.status.idle": "2022-01-09T06:00:55.642695Z",
          "shell.execute_reply.started": "2022-01-09T06:00:55.628386Z",
          "shell.execute_reply": "2022-01-09T06:00:55.641715Z"
        },
        "trusted": true,
        "id": "9jOCTCs94uq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.box(monthly_df[12:], x=\"month\", y=\"cpi_pct_yoy\", points = \"all\", template = \"presentation\",)\n",
        "fig.update_layout(\n",
        "    xaxis = dict(\n",
        "        tickmode = 'linear',))\n",
        "\n",
        "fig = px.box(monthly_df[12:], x=\"quarter\", y=\"cpi_pct_yoy\", points = \"all\", template = \"presentation\")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:00:55.647245Z",
          "iopub.execute_input": "2022-01-09T06:00:55.647779Z",
          "iopub.status.idle": "2022-01-09T06:00:55.73372Z",
          "shell.execute_reply.started": "2022-01-09T06:00:55.647701Z",
          "shell.execute_reply": "2022-01-09T06:00:55.732897Z"
        },
        "trusted": true,
        "id": "dwShS8k04uq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the Core CPI change in an annualized basis, we can see that the earlier parts of the year showed more significant increases in Core CPI. While in the last few months, there are more outliers (below the lower whiskers) meaning that they showed little change from the previous year.<br> We can further explore the volatility of the change in Core CPI YoY."
      ],
      "metadata": {
        "id": "JQ5vHwrU4uq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.bar(\n",
        "    data_frame=monthly_df.groupby(['month']).std().reset_index(),\n",
        "    x=\"month\",\n",
        "    y=\"cpi_pct_yoy\", text=\"cpi_pct_yoy\"\n",
        ").update_traces(texttemplate='%{text:0.3f}', textposition='outside').update_xaxes(nticks=13)\n",
        "fig.show()\n",
        "\n",
        "fig = px.bar(\n",
        "    data_frame=monthly_df.groupby(['quarter']).std().reset_index(),\n",
        "    x=\"quarter\",\n",
        "    y=\"cpi_pct_yoy\", text=\"cpi_pct_yoy\").update_traces(texttemplate='%{text:0.3f}', textposition='outside').update_xaxes(nticks=5)\n",
        "fig.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:00:55.73496Z",
          "iopub.execute_input": "2022-01-09T06:00:55.735204Z",
          "iopub.status.idle": "2022-01-09T06:00:55.873175Z",
          "shell.execute_reply.started": "2022-01-09T06:00:55.735173Z",
          "shell.execute_reply": "2022-01-09T06:00:55.872356Z"
        },
        "trusted": true,
        "id": "x79TXq5T4uq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forecasting Inflation"
      ],
      "metadata": {
        "id": "Vxmx3dWN4uq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "monthly_raw.shape\n",
        "monthly_raw.head(5)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:00:55.874457Z",
          "iopub.execute_input": "2022-01-09T06:00:55.87472Z",
          "iopub.status.idle": "2022-01-09T06:00:55.896546Z",
          "shell.execute_reply.started": "2022-01-09T06:00:55.874689Z",
          "shell.execute_reply": "2022-01-09T06:00:55.896016Z"
        },
        "trusted": true,
        "id": "kzq_XnaK4uq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set date as the dataframe's index."
      ],
      "metadata": {
        "id": "9ugnDnoz4uq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cpi = monthly_raw.set_index('DATE')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:00:55.897438Z",
          "iopub.execute_input": "2022-01-09T06:00:55.897888Z",
          "iopub.status.idle": "2022-01-09T06:00:55.903253Z",
          "shell.execute_reply.started": "2022-01-09T06:00:55.897851Z",
          "shell.execute_reply": "2022-01-09T06:00:55.902463Z"
        },
        "trusted": true,
        "id": "sy5n-mMJ4uq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ARIMA Implementation\n",
        "<a id = \"arima\"></a>\n",
        "As the problem is a time-series forecasting, Autoregressive Integrated Moving Average (ARIMA) that combines AR and MA models can be used to forecast future trends/values.<br> Some upsides of using ARIMA include its interpretability, ease of implementation and may even work better for relatively short series such as this case where the number of observation is not sufficient to apply more sophiscated models.<br> On the other hand, one limitation of ARIMA models is the assumption of constant variance and in financial time-series, most data exhibit volatility, asymmetries, irregular time intervals, sudden outbreaks, thus this model usually perform poorly on financial time series data (Petrica et al., 2016).\n",
        "\n",
        "**References**:<br>\n",
        "- <a src=\"https://www.datasciencecentral.com/profiles/blogs/arima-sarima-vs-lstm-with-ensemble-learning-insights-for-time-ser\"> ARIMA/SARIMA vs LSTM with Ensemble learning Insights for Time Series Data by Sharmistha Chatterjee\n",
        "-Limitation of ARIMA models in financial and monetary economics by Petrica et al., (2016)\n",
        "\n",
        "\n",
        "### Time Series Decomposition\n",
        "Decompose the data into trend, seasonal and residual components"
      ],
      "metadata": {
        "id": "ZyXv7S0S4uq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cpi['ccpi'].plot()\n",
        "fig = seasonal_decompose(df_cpi['ccpi'], model='additive').plot()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:00:55.904644Z",
          "iopub.execute_input": "2022-01-09T06:00:55.905032Z",
          "iopub.status.idle": "2022-01-09T06:00:56.911711Z",
          "shell.execute_reply.started": "2022-01-09T06:00:55.90499Z",
          "shell.execute_reply": "2022-01-09T06:00:56.91073Z"
        },
        "trusted": true,
        "id": "HaMPFzF54uq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data shows a clear upward trend and is not stationary. As one of the key assumptions of the ARIMA model is that the time-series is stationary, we need to correct the non-stationarity later."
      ],
      "metadata": {
        "id": "4MIGXUWH4uq_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting the Data\n",
        "As the dataset is small, we will use the <b>last 12 months as the out of sample test dataset</b>."
      ],
      "metadata": {
        "id": "Fho_csQl4uq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split_point = len(df_cpi) - 12\n",
        "train, test = df_cpi[0:split_point], df_cpi[split_point:]\n",
        "print('Training dataset: %d, Test dataset: %d' % (len(train), len(test)))\n",
        "train['ccpi'].plot()\n",
        "test['ccpi'].plot()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:00:56.913274Z",
          "iopub.execute_input": "2022-01-09T06:00:56.913684Z",
          "iopub.status.idle": "2022-01-09T06:00:57.17632Z",
          "shell.execute_reply.started": "2022-01-09T06:00:56.913622Z",
          "shell.execute_reply": "2022-01-09T06:00:57.175455Z"
        },
        "trusted": true,
        "id": "H1R2LDJy4uq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the plot, we can see that 2020 showed a dip due to the pandemic restrictions. The orange line indicates the Test set.\n",
        "\n",
        "### Take first differences\n",
        "Here, we are finding the number of optimal differencing to remove unit root so that the time-series is stationary. This is done by using diff() function and testing with Augmented Dickey-Fuller test."
      ],
      "metadata": {
        "id": "F0Diz8844uq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diff = train['ccpi'].diff()\n",
        "plt.plot(diff)\n",
        "plt.show()\n",
        "diff = diff.dropna()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:00:57.178028Z",
          "iopub.execute_input": "2022-01-09T06:00:57.178334Z",
          "iopub.status.idle": "2022-01-09T06:00:57.409618Z",
          "shell.execute_reply.started": "2022-01-09T06:00:57.178294Z",
          "shell.execute_reply": "2022-01-09T06:00:57.409079Z"
        },
        "trusted": true,
        "id": "1AK8OdaT4urA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Augmented Dickeyâ€“Fuller test\n",
        "With the small p-value, a 1 differencing is enough to remove unit root and make the series stationary."
      ],
      "metadata": {
        "id": "AA62iQVg4urA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adf_test(df):\n",
        "    result = adfuller(df.values)\n",
        "    if result[1] > 0.05:\n",
        "        print(\"Series is not stationary\")\n",
        "    else:\n",
        "        print(\"Series is stationary\")\n",
        "\n",
        "adf_test(diff)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:00:57.410821Z",
          "iopub.execute_input": "2022-01-09T06:00:57.411195Z",
          "iopub.status.idle": "2022-01-09T06:00:57.430002Z",
          "shell.execute_reply.started": "2022-01-09T06:00:57.411155Z",
          "shell.execute_reply": "2022-01-09T06:00:57.429444Z"
        },
        "trusted": true,
        "id": "QdVuaCGe4urA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot ACF and PACF\n",
        "Now, we need to find the optimal p and q using acf and pacf plot. Where p is the number of lags and q is the order of the MA term. <br><br>\n",
        "Finding the order of Auto Regressive Term (p)\n",
        "- PACF lag 1 is significant"
      ],
      "metadata": {
        "id": "s_gSpelB4urA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_pacf(diff.values)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:00:57.431177Z",
          "iopub.execute_input": "2022-01-09T06:00:57.431552Z",
          "iopub.status.idle": "2022-01-09T06:00:57.773923Z",
          "shell.execute_reply.started": "2022-01-09T06:00:57.431512Z",
          "shell.execute_reply": "2022-01-09T06:00:57.773325Z"
        },
        "trusted": true,
        "id": "OWm_CggI4urB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding the order of Moving Average Term (q)<br>\n",
        "- q = 1 and 2 is significant, try conservative take of q = 1."
      ],
      "metadata": {
        "id": "VWwjmOrM4urB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_acf(diff.values)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:00:57.774879Z",
          "iopub.execute_input": "2022-01-09T06:00:57.775575Z",
          "iopub.status.idle": "2022-01-09T06:00:58.110888Z",
          "shell.execute_reply.started": "2022-01-09T06:00:57.775544Z",
          "shell.execute_reply": "2022-01-09T06:00:58.10972Z"
        },
        "trusted": true,
        "id": "_Nkc1Zhk4urB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the model\n",
        "Since CPI exhibits exponential growth (variance increases), we build the model on the ln(CPI) e.g. converting the raw values to log values.<br>\n",
        "As earlier discovered, the ARIMA model parameters will be set as 1,1,1."
      ],
      "metadata": {
        "id": "ZYcKa7884urG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arima_model = ARIMA(np.log(train['ccpi']), order = (1,1,1))\n",
        "\n",
        "arima_fit = arima_model.fit()\n",
        "arima_fit.summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:00:58.112386Z",
          "iopub.execute_input": "2022-01-09T06:00:58.112989Z",
          "iopub.status.idle": "2022-01-09T06:00:58.280612Z",
          "shell.execute_reply.started": "2022-01-09T06:00:58.112941Z",
          "shell.execute_reply": "2022-01-09T06:00:58.279579Z"
        },
        "trusted": true,
        "id": "rOzLF1ic4urG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forecast for November 2020 - October 2021\n",
        "Forecast for the next 12 months (12 out-of-sample)"
      ],
      "metadata": {
        "id": "DK5pasv_4urG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "forecast = arima_fit.forecast(steps=12)\n",
        "forecast = np.exp(forecast)\n",
        "\n",
        "plt.plot(forecast, color = 'red')\n",
        "\n",
        "pct_chg = ((forecast[-1] - df_cpi.iloc[-12]['ccpi'])/df_cpi.iloc[-12]['ccpi']) * 100\n",
        "print('The forecasted U.S. Core Consumer Price Index (CPI) YoY is ' , round(pct_chg,2))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:00:58.28214Z",
          "iopub.execute_input": "2022-01-09T06:00:58.283086Z",
          "iopub.status.idle": "2022-01-09T06:00:58.564548Z",
          "shell.execute_reply.started": "2022-01-09T06:00:58.28304Z",
          "shell.execute_reply": "2022-01-09T06:00:58.56372Z"
        },
        "trusted": true,
        "id": "tAx-f5fC4urG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating the ARIMA model\n",
        "- with RMSE and\n",
        "- Mean of observed y - predicted y\n",
        "\n",
        "Based on the Mean Error, the ARIMA model overestimates the Core CPI value by an average of 0.25."
      ],
      "metadata": {
        "id": "12w63tn14urG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mse = mean_squared_error(test['ccpi'].values, forecast[:12])\n",
        "print('MSE: ', mse)\n",
        "model_error = test['ccpi'] - forecast\n",
        "print('Mean Model Error: ', model_error.mean())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:00:58.565865Z",
          "iopub.execute_input": "2022-01-09T06:00:58.566095Z",
          "iopub.status.idle": "2022-01-09T06:00:58.575341Z",
          "shell.execute_reply.started": "2022-01-09T06:00:58.566066Z",
          "shell.execute_reply": "2022-01-09T06:00:58.573755Z"
        },
        "trusted": true,
        "id": "aSOw-fKv4urG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-Step Forecast for November 2021"
      ],
      "metadata": {
        "id": "Fz6ozdg24urH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arima_model = ARIMA(np.log(test['ccpi']), order = (1,1,1),freq=test.index.inferred_freq)\n",
        "\n",
        "arima_fit = arima_model.fit()\n",
        "\n",
        "forecast = arima_fit.forecast(steps=1)\n",
        "forecast = np.exp(forecast)\n",
        "\n",
        "print('The Core CPI value for the month November 2021 predicted by ARIMA model is', round(forecast[0],2))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:00:58.576892Z",
          "iopub.execute_input": "2022-01-09T06:00:58.577133Z",
          "iopub.status.idle": "2022-01-09T06:00:58.756775Z",
          "shell.execute_reply.started": "2022-01-09T06:00:58.577104Z",
          "shell.execute_reply": "2022-01-09T06:00:58.755816Z"
        },
        "trusted": true,
        "id": "rPab0RQY4urH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall, we can see that ARIMA performed badly as there was a greater increase in the later months of 2021. We can improve the model by including other features and using multivariate forecasting.\n"
      ],
      "metadata": {
        "id": "tzUqJ9OZ4urH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Univariate Time Series Forecasting with LSTM\n",
        "<a id = \"ulstm\"></a>\n",
        "Long Short-Term Memory (LSTM) Recurrent Neural Network are popular in making predictions based on time series data, due to the lags of unknown duration. LSTM addresses that issue of vanishing gradient with multiple switch gates to remember longer time steps. Hence the past inputs to the model leaves a footprint. With LSTMs, there is no need to keep a finite number of states beforehand as required in the Hhidden Markov model. Common limitations (risk) of LSTM is that it is easy to overfit and hard to train in a sense that it takes alot of resources (computing power) to train these models fast, requiring memory-bandwidth-bound computation.\n",
        "\n",
        "As the problem requires foreccasting the Core CPI value of the January 2021, we can use LSTM to predict a one-step out forecast, using 12 input time steps.\n",
        "\n",
        "The LSTM architecture will be a simple, vanilla LSTM with one hidden layer with default activation tanh.\n",
        "\n",
        "**References**:<br>\n",
        "[The fall of RNN / LSTM](https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0) by Eugenio Culurciello <br>\n",
        "[Essentials of Deep Learning : Introduction to Long Short Term Memory](https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/)"
      ],
      "metadata": {
        "id": "LMwVtEr74urH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scaling and Data Preparation\n",
        "- Scaling with Min-Max Normalization\n",
        "- Split the univariate sequence into samples with 12 steps in 1 step out\n",
        "\n",
        "**References**<br>\n",
        "The split sequence code is based on Machine Learning Mastery's code, available [here](https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/)."
      ],
      "metadata": {
        "id": "N31-rwDM4urH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "dataset = scaler.fit_transform(ccpi.reshape(-1,1))\n",
        "\n",
        "# split a univariate sequence into samples\n",
        "def split_sequence(sequence, n_steps):\n",
        "    X, y = list(), list()\n",
        "    for i in range(len(sequence)):\n",
        "        # find the end of this pattern\n",
        "        end_ix = i + n_steps\n",
        "        # check if we are beyond the sequence\n",
        "        if end_ix > len(sequence)-1:\n",
        "            break\n",
        "        # gather input and output parts of the pattern\n",
        "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
        "        X.append(seq_x)\n",
        "        y.append(seq_y)\n",
        "    return np.array(X), np.array(y)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:00:58.759041Z",
          "iopub.execute_input": "2022-01-09T06:00:58.759507Z",
          "iopub.status.idle": "2022-01-09T06:00:58.769277Z",
          "shell.execute_reply.started": "2022-01-09T06:00:58.759456Z",
          "shell.execute_reply": "2022-01-09T06:00:58.768404Z"
        },
        "trusted": true,
        "id": "NEsRbuJJ4urH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As above, we create a data structure with 12 timesteps and 1 output."
      ],
      "metadata": {
        "id": "aP-JHprM4urH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_steps_in = 12\n",
        "\n",
        "train, test = dataset[0:310], dataset[310:len(dataset),:]\n",
        "\n",
        "trainX, trainY = split_sequence(train, n_steps_in)\n",
        "testX, testY = split_sequence(test, n_steps_in)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:00:58.771228Z",
          "iopub.execute_input": "2022-01-09T06:00:58.771988Z",
          "iopub.status.idle": "2022-01-09T06:00:58.786423Z",
          "shell.execute_reply.started": "2022-01-09T06:00:58.771907Z",
          "shell.execute_reply": "2022-01-09T06:00:58.785719Z"
        },
        "trusted": true,
        "id": "LP9SY8ce4urI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the Model\n",
        "\n",
        "Build vanilla model with a single layer, no dropout regularization and no Early Stopping. The number of neurons that will be used is 100, for high dimensionality (so the model can capture the trends). Most parameters are chosen through a fine-tuning and trial-and-error approach.\n",
        "\n",
        "The number of layer, 1 is chosen because of the complexity of the problem. <br>No dropout regularization due to the simple complexity nature (there is 1 layer only, and adding dropout would cause important information to be lost) of the built neural network and dataset size. Dropout regularization at optimal levels 0.1/0.2 for LSTM were trialed and significantly decreased the performance of prediction on the test set. Dropout erase important context information, especially in this problem with limited timesteps and 1 layer. Furthermore, train loss and validation loss are carefully monitored for any overfitting.<br>\n",
        "Similarly, a small learning rate of 0.001 is used due to size of the neural network and small data size.\n",
        "\n",
        "The batch size is set as 100 epochs. Batch size fine-tuning is done based on the observation of model's peformance.\n"
      ],
      "metadata": {
        "id": "DP1KRisk4urI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_features = trainX.shape[2]\n",
        "\n",
        "uni_model = Sequential()\n",
        "\n",
        "# Adding the LSTM layer\n",
        "uni_model.add(LSTM(64, input_shape=(trainX.shape[1], n_features)))\n",
        "\n",
        "# Adding the output layer\n",
        "uni_model.add(Dense(1))\n",
        "\n",
        "uni_model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001),\n",
        "              loss = 'mean_squared_error', metrics=['mean_absolute_error'])\n",
        "\n",
        "fit = uni_model.fit(trainX,\n",
        "          trainY, validation_data = (testX, testY),\n",
        "          epochs = 100, batch_size=1,\n",
        "          verbose = 0)\n",
        "\n",
        "\n",
        "# Check for overfitting\n",
        "plt.plot(fit.history['loss'], label = 'training', color = 'Blue')\n",
        "plt.plot(fit.history['val_loss'], label = 'validation', color = 'Red')\n",
        "plt.legend()\n",
        "plt.show"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:00:58.787737Z",
          "iopub.execute_input": "2022-01-09T06:00:58.789816Z",
          "iopub.status.idle": "2022-01-09T06:02:41.044517Z",
          "shell.execute_reply.started": "2022-01-09T06:00:58.78976Z",
          "shell.execute_reply": "2022-01-09T06:02:41.043697Z"
        },
        "trusted": true,
        "id": "3I5N815d4urI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see a slight overfit.\n",
        "\n",
        "**Note**: The validation loss shows significant spike and fluctuations due to the small batch_size passed in.\n",
        "\n",
        "### Predictions on Test Set\n",
        "- Make predictions on both the train set and test set\n",
        "- Inverse transform from normalized back to the original value."
      ],
      "metadata": {
        "id": "cVv1sHiD4urI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainPredict = uni_model.predict(trainX)\n",
        "testPredict = uni_model.predict(testX)\n",
        "\n",
        "Ytrain_hat = scaler.inverse_transform(trainPredict)\n",
        "Ytrain_actual = scaler.inverse_transform(trainY)\n",
        "Ytest_hat = scaler.inverse_transform(testPredict)\n",
        "Ytest_actual = scaler.inverse_transform(testY)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:02:41.045952Z",
          "iopub.execute_input": "2022-01-09T06:02:41.046847Z",
          "iopub.status.idle": "2022-01-09T06:02:41.587397Z",
          "shell.execute_reply.started": "2022-01-09T06:02:41.046805Z",
          "shell.execute_reply": "2022-01-09T06:02:41.586523Z"
        },
        "trusted": true,
        "id": "bldLEv1A4urI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating the univariate LSTM\n",
        "\n",
        "Similar to the ARIMA, we will use MSE and Mean Model Error to evaluate the model's forecasting power."
      ],
      "metadata": {
        "id": "4MTSNDL94urI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainScore = mean_squared_error(Ytrain_actual, Ytrain_hat[:,0])\n",
        "print('Train Score: %.2f MSE' % (trainScore))\n",
        "testScore = mean_squared_error(Ytest_actual, Ytest_hat[:,0])\n",
        "print('Test Score: %.2f MSE' % (testScore))\n",
        "\n",
        "model_error = Ytest_actual - Ytest_hat[:,0]\n",
        "print('Mean Model Error: ', model_error.mean())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:02:41.59175Z",
          "iopub.execute_input": "2022-01-09T06:02:41.592182Z",
          "iopub.status.idle": "2022-01-09T06:02:41.599912Z",
          "shell.execute_reply.started": "2022-01-09T06:02:41.592149Z",
          "shell.execute_reply": "2022-01-09T06:02:41.59907Z"
        },
        "trusted": true,
        "id": "-Ry2FpNP4urJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model showed significant improvement compared to ARIMA model with MSE of 1.70 and Mean Model Error of 0.78 which indicates the model gives a slight underestimation. Based on the MSE, the model has an average error of sqrt(1.70) = 1.30."
      ],
      "metadata": {
        "id": "ZIY1g4gt4urJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Now, we can plot the actual 2020 Core CPI against the forecasted 2020 Core CPI."
      ],
      "metadata": {
        "id": "wSPF2TVb4urJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "observed = df_cpi.loc['2020-11-01':'2021-10-01',['ccpi']]\n",
        "observed.plot(color = 'SteelBlue', title = 'Actual', legend = False)\n",
        "plt.show()\n",
        "\n",
        "predicted = pd.DataFrame(Ytest_hat, index=pd.date_range('2020-11-01',periods=12,freq='M'))\n",
        "predicted.plot(color = 'Firebrick', title = 'Forecasted', legend = False)\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:02:41.601324Z",
          "iopub.execute_input": "2022-01-09T06:02:41.601732Z",
          "iopub.status.idle": "2022-01-09T06:02:42.054292Z",
          "shell.execute_reply.started": "2022-01-09T06:02:41.601702Z",
          "shell.execute_reply": "2022-01-09T06:02:42.053714Z"
        },
        "trusted": true,
        "id": "Y81M65wV4urJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above line chart, we can see that the univariate LSTM is able to \"approximate\" the shape of the Core CPI (though lagging) and underestimates the value.\n",
        "\n",
        "### Forecast for November 2021\n",
        "\n",
        "We input the last 12 observations (12 steps in) into the model for it to forecast 1 step out which will be the Core CPI value for the month of November 2021.\n",
        "\n",
        "Then, calculate the year-on-year percentage change manually."
      ],
      "metadata": {
        "id": "xV6bWntl4urJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_input = np.array(dataset[-12:])\n",
        "x_input = x_input.reshape((1, n_steps_in, n_features))\n",
        "\n",
        "forecast_normalized = uni_model.predict(x_input)\n",
        "\n",
        "forecast = scaler.inverse_transform(forecast_normalized)\n",
        "print('The Core CPI value for the month Nov 2021 predicted by LSTM is ', forecast[0][0])\n",
        "\n",
        "pct_chg = ((forecast[0][0] - df_cpi.iloc[-12]['ccpi'])/df_cpi.iloc[-12]['ccpi']) * 100\n",
        "print('The forecasted U.S. Core Consumer Price Index (CPI) YoY is ' , round(pct_chg,2))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:02:42.055368Z",
          "iopub.execute_input": "2022-01-09T06:02:42.056141Z",
          "iopub.status.idle": "2022-01-09T06:02:42.118743Z",
          "shell.execute_reply.started": "2022-01-09T06:02:42.056104Z",
          "shell.execute_reply": "2022-01-09T06:02:42.117831Z"
        },
        "trusted": true,
        "id": "v8IYji1x4urJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multivariate Time Series Forecasting with LSTMs\n",
        "<a id = \"mlstm\"></a>\n",
        "### Data Preparation\n",
        "- Feature Selection with Granger Causality Test\n",
        "- Scaling with Min-Max normalization\n",
        "- Split multivariate sequence into samples with 12 steps in and 1 step out (<i>Code referenced from machinelearningmastery</i>)\n"
      ],
      "metadata": {
        "id": "dSN7w2-04urJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Feature Selection\n",
        "In order to see which features is useful for forecasting core CPI, we will be using the Granger Causality test.\n",
        "One of the key assumptions before using this test requires the data to be stationary. Thus, we would take the first differences for each features and use the same ADF test function to check for stationarity."
      ],
      "metadata": {
        "id": "vF8ZIvLL4urK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "monthly_df_stationary = df_cpi.diff().dropna()\n",
        "monthly_df_stationary = monthly_df_stationary.rename_axis('indicator', axis=1)\n",
        "fig = px.line(monthly_df_stationary.iloc[:,0:10], facet_col=\"indicator\", facet_col_wrap=1)\n",
        "fig.update_yaxes(visible=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:02:42.119922Z",
          "iopub.execute_input": "2022-01-09T06:02:42.120373Z",
          "iopub.status.idle": "2022-01-09T06:02:42.528443Z",
          "shell.execute_reply.started": "2022-01-09T06:02:42.12034Z",
          "shell.execute_reply": "2022-01-09T06:02:42.527667Z"
        },
        "trusted": true,
        "id": "fjZhmMvf4urK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for indi in monthly_df_stationary:\n",
        "    print('ADF Test: ', indi)\n",
        "    adf_test(monthly_df_stationary[[indi]])\n",
        "\n",
        "monthly_df_stationary[['m2']] = monthly_df_stationary[['m2']].diff().dropna()\n",
        "monthly_df_stationary[['tcs']] = monthly_df_stationary[['tcs']].diff().dropna()\n",
        "monthly_df_stationary[['ccpi']] = monthly_df_stationary[['ccpi']].diff().dropna()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:02:42.529721Z",
          "iopub.execute_input": "2022-01-09T06:02:42.529926Z",
          "iopub.status.idle": "2022-01-09T06:02:42.704689Z",
          "shell.execute_reply.started": "2022-01-09T06:02:42.5299Z",
          "shell.execute_reply": "2022-01-09T06:02:42.703811Z"
        },
        "trusted": true,
        "id": "8Zvf9hfc4urK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After first differencing, M2, TCS and CCPI are still not stationary, so we will take the second differencing for these indicators and check if it still contains unit root."
      ],
      "metadata": {
        "id": "Jn8WZdb44urK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop any NaNs first\n",
        "monthly_df_stationary = monthly_df_stationary.dropna()\n",
        "\n",
        "for indi in monthly_df_stationary:\n",
        "    print('ADF Test: ', indi)\n",
        "    adf_test(monthly_df_stationary[[indi]])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:02:42.70623Z",
          "iopub.execute_input": "2022-01-09T06:02:42.706524Z",
          "iopub.status.idle": "2022-01-09T06:02:42.872327Z",
          "shell.execute_reply.started": "2022-01-09T06:02:42.706483Z",
          "shell.execute_reply": "2022-01-09T06:02:42.871388Z"
        },
        "trusted": true,
        "id": "HCEHUPB-4urK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "M2, TCS and CCPI are now stationary after second differencing. We can proceed to use Grangers Causality Test to investigate causality between our features and Core CPI. Granger causality is a statistical concept of causality that is based on prediction and is highly relevant in financial economics.\n",
        "\n",
        "The code below is taken from [stackoverflow](https://stackoverflow.com/questions/58005681/is-it-possible-to-run-a-vector-autoregression-analysis-on-a-large-gdp-data-with)."
      ],
      "metadata": {
        "id": "FjSiJmg64urK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "maxlag=12\n",
        "test = 'ssr_chi2test'\n",
        "\n",
        "def grangers_causation_matrix(data, variables, test='ssr_chi2test', verbose=False):\n",
        "\n",
        "    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
        "    for c in df.columns:\n",
        "        for r in df.index:\n",
        "            test_result = grangercausalitytests(data[[r, c]], maxlag=maxlag, verbose=False)\n",
        "            p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\n",
        "            if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n",
        "            min_p_value = np.min(p_values)\n",
        "            df.loc[r, c] = min_p_value\n",
        "    df.columns = [var + '_x' for var in variables]\n",
        "    df.index = [var + '_y' for var in variables]\n",
        "    return df\n",
        "\n",
        "grangers_causation_matrix(monthly_df_stationary, variables = monthly_df_stationary.columns)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:02:42.874627Z",
          "iopub.execute_input": "2022-01-09T06:02:42.874872Z",
          "iopub.status.idle": "2022-01-09T06:02:47.433037Z",
          "shell.execute_reply.started": "2022-01-09T06:02:42.874844Z",
          "shell.execute_reply": "2022-01-09T06:02:47.432475Z"
        },
        "trusted": true,
        "id": "015uzwr-4urK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we can focus on just the **last row**, and we will be using a significance level of 0.05 like earlier, hence any p-values that are less than 0.05, we can reject the null hypothesis and conclude that the feature granger causes Core CPI.<br><br> Real Effective Exchange Rate, 10Y Treasury Yield and Fed Rate are not significant, thus we can first proceed to exclude them from our future models, dropping them from the dataframe."
      ],
      "metadata": {
        "id": "LauJllei4urL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feat_df = df_cpi.drop(['reer', 'ir','ffer'], axis = 1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:02:47.434197Z",
          "iopub.execute_input": "2022-01-09T06:02:47.434423Z",
          "iopub.status.idle": "2022-01-09T06:02:47.439893Z",
          "shell.execute_reply.started": "2022-01-09T06:02:47.434394Z",
          "shell.execute_reply": "2022-01-09T06:02:47.438853Z"
        },
        "trusted": true,
        "id": "DwiSOQff4urL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Scaling with Min-Max Normalization"
      ],
      "metadata": {
        "id": "xHKqHo8r4urL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaled = scaler.fit_transform(feat_df)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:02:47.441464Z",
          "iopub.execute_input": "2022-01-09T06:02:47.441829Z",
          "iopub.status.idle": "2022-01-09T06:02:47.456142Z",
          "shell.execute_reply.started": "2022-01-09T06:02:47.441786Z",
          "shell.execute_reply": "2022-01-09T06:02:47.455437Z"
        },
        "trusted": true,
        "id": "e3-ZluZd4urL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert the array back to dataframe for resuability purposes"
      ],
      "metadata": {
        "id": "lakOrhuY4urL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_df = pd.DataFrame(scaled, columns=feat_df.columns, index=feat_df.index)\n",
        "scaled_df.head(5)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:02:47.457487Z",
          "iopub.execute_input": "2022-01-09T06:02:47.457894Z",
          "iopub.status.idle": "2022-01-09T06:02:47.478073Z",
          "shell.execute_reply.started": "2022-01-09T06:02:47.457852Z",
          "shell.execute_reply": "2022-01-09T06:02:47.477226Z"
        },
        "trusted": true,
        "id": "3RpRgHth4urL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Splitting Multivariate Sequences into Samples"
      ],
      "metadata": {
        "id": "zIdG5KFI4urL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
        "    X, y = list(), list()\n",
        "    for i in range(len(sequences)):\n",
        "        #find end of pattern\n",
        "        end_ix = i + n_steps_in\n",
        "        out_end_ix = end_ix + n_steps_out - 1\n",
        "\n",
        "        #check if we are beyond the dataset\n",
        "        if out_end_ix > len(sequences):\n",
        "            break\n",
        "\n",
        "        #gather input and output\n",
        "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix - 1:out_end_ix, -1]\n",
        "        X.append(seq_x)\n",
        "        y.append(seq_y)\n",
        "\n",
        "    return np.array(X), np.array(y)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:02:47.479716Z",
          "iopub.execute_input": "2022-01-09T06:02:47.480007Z",
          "iopub.status.idle": "2022-01-09T06:02:47.486826Z",
          "shell.execute_reply.started": "2022-01-09T06:02:47.479967Z",
          "shell.execute_reply": "2022-01-09T06:02:47.486179Z"
        },
        "trusted": true,
        "id": "vQe1jBtD4urL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to the univariate model, we will leave the last 12 months as the test set. We configure the inputs to be from 1994 to 2019 (where 2000 will contain the first 12 time steps in) for the train set and 2019 to 2020 (where 2019 will contain the first 12 steps in) for the test set."
      ],
      "metadata": {
        "id": "fsTd53Xe4urM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#inputs\n",
        "in_cpi = np.array(scaled_df.loc['1994-01-01':'2019-11-01', ['ccpi']])\n",
        "in_ur = np.array(scaled_df.loc['1994-01-01':'2019-11-01', ['unrate']])\n",
        "in_m2  = np.array(scaled_df.loc['1994-01-01':'2019-11-01', ['m2']])\n",
        "in_pce = np.array(scaled_df.loc['1994-01-01':'2019-11-01', ['pce']])\n",
        "in_dspic  = np.array(scaled_df.loc['1994-01-01':'2019-11-01', ['dspic']])\n",
        "in_tcs = np.array(scaled_df.loc['1994-01-01':'2019-11-01', ['tcs']])\n",
        "in_psr = np.array(scaled_df.loc['1994-01-01':'2019-11-01', ['psr']])\n",
        "in_ind = np.array(scaled_df.loc['1994-01-01':'2019-11-01', ['indpro']])\n",
        "\n",
        "test_cpi = np.array(scaled_df.loc['2019-12-01':'2021-10-01', ['ccpi']])\n",
        "test_ur = np.array(scaled_df.loc['2019-12-01':'2021-10-01', ['unrate']])\n",
        "test_pce = np.array(scaled_df.loc['2019-12-01':'2021-10-01', ['pce']])\n",
        "test_dspic  = np.array(scaled_df.loc['2019-12-01':'2021-10-01', ['dspic']])\n",
        "test_m2  = np.array(scaled_df.loc['2019-12-01':'2021-10-01', ['m2']])\n",
        "test_tcs = np.array(scaled_df.loc['2019-12-01':'2021-10-01', ['tcs']])\n",
        "test_psr = np.array(scaled_df.loc['2019-12-01':'2021-10-01', ['psr']])\n",
        "test_ind = np.array(scaled_df.loc['2019-12-01':'2021-10-01', ['indpro']])\n",
        "\n",
        "#output\n",
        "trainoutput_cpi = in_cpi\n",
        "testoutput_cpi = test_cpi"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:02:47.488103Z",
          "iopub.execute_input": "2022-01-09T06:02:47.488317Z",
          "iopub.status.idle": "2022-01-09T06:02:47.535811Z",
          "shell.execute_reply.started": "2022-01-09T06:02:47.488292Z",
          "shell.execute_reply": "2022-01-09T06:02:47.534963Z"
        },
        "trusted": true,
        "id": "uwYa9Suh4urM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we reshape the input and output data into rows, columns format."
      ],
      "metadata": {
        "id": "C7nLUv3I4urM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "in_cpi = in_cpi.reshape((len(in_cpi), 1))\n",
        "in_ur = in_ur.reshape((len(in_ur), 1))\n",
        "in_pce = in_pce.reshape((len(in_pce), 1))\n",
        "in_dspic = in_dspic.reshape((len(in_dspic), 1))\n",
        "in_m2  = in_m2.reshape((len(in_m2), 1))\n",
        "in_tcs = in_tcs.reshape((len(in_tcs), 1))\n",
        "in_psr = in_psr.reshape((len(in_psr), 1))\n",
        "in_ind = in_ind.reshape((len(in_ind), 1))\n",
        "\n",
        "test_cpi = test_cpi.reshape((len(test_cpi), 1))\n",
        "test_ur = test_ur.reshape((len(test_ur), 1))\n",
        "test_pce = test_pce.reshape((len(test_pce), 1))\n",
        "test_dspic = test_dspic.reshape((len(test_dspic), 1))\n",
        "test_m2  = test_m2.reshape((len(test_m2), 1))\n",
        "test_tcs = test_tcs.reshape((len(test_tcs), 1))\n",
        "test_psr = test_psr.reshape((len(test_psr), 1))\n",
        "test_ind = test_ind.reshape((len(test_ind), 1))\n",
        "\n",
        "trainoutput_cpi = trainoutput_cpi.reshape((len(trainoutput_cpi), 1))\n",
        "testoutput_cpi = testoutput_cpi.reshape((len(testoutput_cpi), 1))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:02:47.536935Z",
          "iopub.execute_input": "2022-01-09T06:02:47.5372Z",
          "iopub.status.idle": "2022-01-09T06:02:47.553133Z",
          "shell.execute_reply.started": "2022-01-09T06:02:47.53717Z",
          "shell.execute_reply": "2022-01-09T06:02:47.552416Z"
        },
        "trusted": true,
        "id": "58SYD0R14urM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we stack all the columns horizontally using numpy hstack to get it ready to be passed into the split sequences function. And pass the defined steps in and the single-step forecast."
      ],
      "metadata": {
        "id": "yNIzed314urM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = np.hstack((in_cpi, in_ur, in_pce, in_dspic, in_m2, in_tcs, in_psr, in_ind, trainoutput_cpi))\n",
        "testset = np.hstack((test_cpi, test_ur, test_pce, test_dspic, test_m2, test_tcs, test_psr, test_ind, testoutput_cpi))\n",
        "\n",
        "n_steps_in = 12\n",
        "n_steps_out = 1\n",
        "\n",
        "trainX, trainy = split_sequences(trainset, n_steps_in, n_steps_out)\n",
        "\n",
        "testX, testy = split_sequences(testset, n_steps_in, n_steps_out)\n",
        "\n",
        "trainX.shape, trainy.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:02:47.554554Z",
          "iopub.execute_input": "2022-01-09T06:02:47.555181Z",
          "iopub.status.idle": "2022-01-09T06:02:47.571012Z",
          "shell.execute_reply.started": "2022-01-09T06:02:47.555146Z",
          "shell.execute_reply": "2022-01-09T06:02:47.570392Z"
        },
        "trusted": true,
        "id": "srHGUh9n4urM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the Model\n",
        "The initial model was trialed on this multivariate with similar parameters as the univariate model but resulted in poor performance. The model was further tuned into a 500 epochs with default batch size of 32 and stacking an additional LSTM layer. A GridSearchCV attempt was done before to find the optimal epochs and batch size.\n",
        "\n",
        "The model showed significant overfitting, thus regularization with EarlyStopping and Dropout were used. The final Dropout was set at 20% and added between the rec and Dense fully output layer. The EarlyStopping patience was set at 50.\n",
        "\n",
        "The hyperparameters were also tuned through manual experimental runs observing the performance.\n",
        "\n",
        "**References**:<br>\n",
        "<li><a src=\"https://stackoverflow.com/questions/48714407/rnn-regularization-which-component-to-regularize/58868383#58868383\"> A comprehensive answer to RNN Regularization"
      ],
      "metadata": {
        "id": "b67nDhlz4urM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_features = trainX.shape[2]\n",
        "\n",
        "multi_model = Sequential()\n",
        "\n",
        "# Adding the LSTM layer and dropout regularizaiton\n",
        "multi_model.add(LSTM(100, return_sequences = True, input_shape=(n_steps_in, n_features)))\n",
        "multi_model.add(LSTM(100))\n",
        "multi_model.add(Dropout(0.2))\n",
        "\n",
        "# Adding output layer\n",
        "multi_model.add(Dense(n_steps_out))\n",
        "\n",
        "multi_model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001),\n",
        "              loss = 'mean_squared_error')\n",
        "\n",
        "earlystop = EarlyStopping(monitor = 'val_loss', patience =50,\n",
        "                  mode = 'min',\n",
        "                  verbose = 0)\n",
        "\n",
        "fit = multi_model.fit(trainX,\n",
        "          trainy, validation_data = (testX, testy),\n",
        "          epochs = 500, verbose=0, callbacks = [earlystop])\n",
        "\n",
        "\n",
        "# Check for overfitting\n",
        "plt.plot(fit.history['loss'], label = 'training', color = 'Blue')\n",
        "plt.plot(fit.history['val_loss'], label = 'validation', color = 'Red')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:02:47.571849Z",
          "iopub.execute_input": "2022-01-09T06:02:47.57209Z",
          "iopub.status.idle": "2022-01-09T06:03:14.269757Z",
          "shell.execute_reply.started": "2022-01-09T06:02:47.572061Z",
          "shell.execute_reply": "2022-01-09T06:03:14.268951Z"
        },
        "trusted": true,
        "id": "yDhsMsUe4urN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: The model was early stopped close to the 170th epoch."
      ],
      "metadata": {
        "id": "aJ_oEWpg4urN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Importance\n",
        "\n",
        "There are a few ways to quantify and identify feature importance that influenced the recurrent neural network. Some of which include Pertubations, [Masking](https://stackoverflow.com/questions/44119207/is-there-any-way-to-get-variable-importance-with-keras)/[LIME](https://arxiv.org/abs/1606.05386), [Permutation Importance](https://www.kaggle.com/cdeotte/lstm-feature-importance)  and [SHAP](https://christophm.github.io/interpretable-ml-book/shap.html) (however, this led to many compatibility issues with tensorflow 2.7.0 on my end).\n",
        "\n",
        "Here, we will be using pertubation effect which is quite similar to Masking and LIME. The idea here involves introducing noise/perturbing each variable with a random normal distribution then caclulate the difference between the perturbed predicted y and original predicted y.\n",
        "\n",
        "**References**<br>\n",
        "More information on pertubation on Neural Networks available [here](https://towardsdatascience.com/perturbation-theory-in-deep-neural-network-dnn-training-adb4c20cab1b) and [here](https://stats.stackexchange.com/questions/191855/variable-importance-in-rnn-or-lstm)."
      ],
      "metadata": {
        "id": "_JKy9hPh4urN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_importance(model, g):\n",
        "    random_ind = np.random.choice(g.shape[0], 100, replace=False) # Randomly generate 100 numbers arange(218)\n",
        "    x = g[random_ind] #  Take 100 random sample from training set\n",
        "    orig_out = model.predict(x)\n",
        "    for i in range(8):  # iterate over the 7 features\n",
        "        new_x = x.copy()\n",
        "        perturbation_in = np.random.normal(0.0, 0.7, size=new_x.shape[:2]) # Draw random samples from normal distribution with sd = 0.7, this value is arbitary and would not affect the order of effect as its just introducing noise.\n",
        "        new_x[:, :, i] = new_x[:, :, i] + perturbation_in\n",
        "        perturbed_out = model.predict(new_x)\n",
        "        effect = ((orig_out - perturbed_out) ** 2).mean() ** 0.5\n",
        "        print(f'Variable {i+1}, Perturbation Effect: {effect:.3f}')\n",
        "\n",
        "feature_importance(multi_model,trainX)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:03:14.270955Z",
          "iopub.execute_input": "2022-01-09T06:03:14.271168Z",
          "iopub.status.idle": "2022-01-09T06:03:16.664861Z",
          "shell.execute_reply.started": "2022-01-09T06:03:14.271142Z",
          "shell.execute_reply": "2022-01-09T06:03:16.664004Z"
        },
        "trusted": true,
        "id": "34pVghGV4urN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the perturbation effect results, we could say that the important features for forecasting Core CPI in this model/dataset is past **Core CPI**, **Personal Consumption Expenditure**, and **M2**. Personal Consumption is one of the most important feature and this is obvious because PCE is also an important metric in determining inflation thus it should be closely correlated with Core CPI.\n",
        "\n",
        "\n",
        "### Predictions on Test Set\n",
        "\n",
        "After prediction, we need to invert the min-max normalization. To do so, we reshape the data back to the original form before the normalization.\n",
        "- Reshape the testX and concatenate with the y-hat (prediction) at the correct positions\n",
        "- Reshape the testX and concatenate with the actual y at the correct positions\n",
        "\n",
        "To match the order of the original dataframe, we concatenate in the order of unemployment_rate, m2, pce, dspic, ffr, psr and cpi."
      ],
      "metadata": {
        "id": "jxrxzqj94urN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testPredict = multi_model.predict(testX)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:03:16.673317Z",
          "iopub.execute_input": "2022-01-09T06:03:16.673731Z",
          "iopub.status.idle": "2022-01-09T06:03:16.738722Z",
          "shell.execute_reply.started": "2022-01-09T06:03:16.673696Z",
          "shell.execute_reply": "2022-01-09T06:03:16.737913Z"
        },
        "trusted": true,
        "id": "8wrcZ_sw4urN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testX = testX.reshape((testX.shape[0], testX.shape[2]*testX.shape[1]))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:03:16.740018Z",
          "iopub.execute_input": "2022-01-09T06:03:16.740249Z",
          "iopub.status.idle": "2022-01-09T06:03:16.745974Z",
          "shell.execute_reply.started": "2022-01-09T06:03:16.740218Z",
          "shell.execute_reply": "2022-01-09T06:03:16.744898Z"
        },
        "trusted": true,
        "id": "Dvhqc-oJ4urN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Invert scaling for Predicted\n",
        "testY_hat = np.concatenate((testX[:, 1:8], testPredict), axis=1)\n",
        "testY_hat = scaler.inverse_transform(testY_hat)\n",
        "\n",
        "testY_hat = testY_hat[:,7]\n",
        "\n",
        "# Invert scaling for Actual\n",
        "testY_actual = np.concatenate((testX[:,1:8], testy), axis=1)\n",
        "testY_actual = scaler.inverse_transform(testY_actual)\n",
        "\n",
        "testY_actual = testY_actual[:,7]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:03:16.747311Z",
          "iopub.execute_input": "2022-01-09T06:03:16.74853Z",
          "iopub.status.idle": "2022-01-09T06:03:16.756062Z",
          "shell.execute_reply.started": "2022-01-09T06:03:16.748492Z",
          "shell.execute_reply": "2022-01-09T06:03:16.755517Z"
        },
        "trusted": true,
        "id": "S38-qSX44urN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating the multivariate LSTM\n",
        "\n",
        "Similar to the previous models, we will use the same metrics: MSE and Mean Model Error."
      ],
      "metadata": {
        "id": "gyIO73Qq4urO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mse = mean_squared_error(testY_actual, testY_hat)\n",
        "print('Test MSE: %.3f' % mse)\n",
        "\n",
        "model_error = testY_actual - testY_hat\n",
        "print('Mean Model Error: ', model_error.mean())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:03:16.757006Z",
          "iopub.execute_input": "2022-01-09T06:03:16.757555Z",
          "iopub.status.idle": "2022-01-09T06:03:16.771319Z",
          "shell.execute_reply.started": "2022-01-09T06:03:16.757522Z",
          "shell.execute_reply": "2022-01-09T06:03:16.770556Z"
        },
        "trusted": true,
        "id": "gLwdxyyM4urO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "observed = df_cpi.loc['2020-11-01':'2021-10-01',['ccpi']]\n",
        "observed.plot(color = 'SteelBlue', title = 'Actual', legend = False)\n",
        "plt.show()\n",
        "\n",
        "predicted = pd.DataFrame(testY_hat, index=pd.date_range('2020-11-01',periods=12,freq='M'))\n",
        "predicted.plot(color = 'Firebrick', title = 'Forecasted', legend = False)\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:03:16.772539Z",
          "iopub.execute_input": "2022-01-09T06:03:16.772794Z",
          "iopub.status.idle": "2022-01-09T06:03:17.215189Z",
          "shell.execute_reply.started": "2022-01-09T06:03:16.772754Z",
          "shell.execute_reply": "2022-01-09T06:03:17.214381Z"
        },
        "trusted": true,
        "id": "kHLo2TSS4urO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As compared to the univariate LSTM, the model had a bad start in January was unable to 'approximate' the actual shape of the Core CPI in 2020."
      ],
      "metadata": {
        "id": "K6XvXTHK4urO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forecast for November 2021"
      ],
      "metadata": {
        "id": "jgZo-CtV4urO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_input = np.array(scaled[-12:])\n",
        "x_input = x_input.reshape((1, n_steps_in, n_features))\n",
        "\n",
        "forecast_normalized = multi_model.predict(x_input)\n",
        "\n",
        "# Manually inverse Min-max normalization\n",
        "max_cpi = df_cpi['ccpi'].max()\n",
        "min_cpi = df_cpi['ccpi'].min()\n",
        "forecast =  max_cpi-forecast_normalized[0][0]/(max_cpi-min_cpi)\n",
        "print('The Core CPI value for the month Nov 2021 predicted by LSTM is ', forecast)\n",
        "\n",
        "pct_chg = ((forecast - df_cpi.iloc[-12]['ccpi'])/df_cpi.iloc[-12]['ccpi']) * 100\n",
        "print('The forecasted U.S. Core Consumer Price Index (CPI) YoY is ' , round(pct_chg,2))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T06:03:17.216342Z",
          "iopub.execute_input": "2022-01-09T06:03:17.21658Z",
          "iopub.status.idle": "2022-01-09T06:03:17.280594Z",
          "shell.execute_reply.started": "2022-01-09T06:03:17.216551Z",
          "shell.execute_reply": "2022-01-09T06:03:17.279731Z"
        },
        "trusted": true,
        "id": "088X-XI14urO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}